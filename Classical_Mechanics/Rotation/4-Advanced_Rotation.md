# Some Advanced Topics on Rotation

In this lecture introduce some advanced concepts related to rotations.

## The Levi-Civita Symbol

You may wonder: how can I remember the position of the three components in the angular velocity matrix? Well, if you stare at the expressions long enough:

$$
\omega_1=\omega_{32}, \quad
\omega_2=\omega_{13}, \quad
\omega_3=\omega_{21}
$$

You recognize that we may write

$$
\omega_i = \omega_{j k}
$$

where $(i j k)$ is a **permutation** of (321). Mathematicians have invented an object called the **Levi-Civita symbol** to create a shorthand
of our conclusion:

$$
\epsilon_{i j k}= \begin{cases}
    1 & (i j k)=(123),(231),(312) \\
    -1 & (i j k)=(132),(213),(321) \\
    0 & \text{Otherwise}
\end{cases}
$$

Then we can write

$$
\omega_i = -\frac{1}{2}\epsilon_{i j k}\omega_{j k}
$$

## The Generator of Rotations

The matrix $\omega_{i j}$ can be written as the sum of three linearly independent parts:

$$
\omega_{i j}
= \omega_1\mathcal{J}_{i j}^{(1)}
+ \omega_2\mathcal{J}_{i j}^{(2)}
+ \omega_3\mathcal{J}_{i j}^{(3)}
$$

Here we introduced three antisymmetric matrices

$$\begin{aligned}
    \mathcal{J}^{(1)}
    &= \begin{bmatrix}
        0 & 0 & 0 \\
        0 & 0 & -1 \\
        0 & 1 & 0
    \end{bmatrix}
    \\
    \mathcal{J}^{(2)}
    &= \begin{bmatrix}
        0 & 0 & 1 \\
        0 & 0 & 0 \\
        -1 & 0 & 0
    \end{bmatrix}
    \\
    \mathcal{J}^{(3)}
    &= \begin{bmatrix}
        0 & -1 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 0
    \end{bmatrix}
\end{aligned}
$$

Now we play some magic: using the **Taylor expansion** of the **exponential function**

$$
e^x = 1 + x + \frac{1}{2!}x^2
+ \frac{1}{3!}x^3 + \cdots
$$

We can replace the $x$ by a *square matrix* $A$ and obtain

$$
e^A = 1 + A + \frac{1}{2!}A^2
+ \frac{1}{3!}A^3 + \cdots
$$

After calculating
$\mathcal{J},\mathcal{J}^2,\mathcal{J}^3,\text{...}$ in the series, you will see a pattern and obtain some exciting results:

$$
\begin{aligned}
    R_x(\alpha) = \exp (\alpha \mathcal{J}^{(1)})
    \\
    R_y(\alpha) = \exp (\alpha \mathcal{J}^{(2)})
    \\
    R_z(\alpha) = \exp (\alpha \mathcal{J}^{(3)})
\end{aligned}
$$

For this reason, the $\mathcal{J}$ matrices are called the **generators** (via exponential functions) of 3D rotations (or using some more fancy terms, of the **3D special orthogonal group $SO(3)$**). The so-called $SO(3)$ group is the collection of all 3D rotations, together with the rule about how to combine them.

After some linear combination (with complex coefficients), the
$\mathcal{J}$ matrices will become the **angular momentum operator** in quantum mechanics.

### EXERCISE

- We define the **commutator** of two square matrices $A,B$ as

    $$
    [A,B]=A B-B A
    $$

    Show that

    $$
    \begin{aligned}
        [\mathcal{J}^{(1)},\mathcal{J}^{(2)}]
        &= \mathcal{J}^{(3)}
        \\
        [\mathcal{J}^{(2)},\mathcal{J}^{(3)}]
        &= \mathcal{J}^{(1)}
        \\
        [\mathcal{J}^{(3)},\mathcal{J}^{(1)}]
        &= \mathcal{J}^{(2)}
    \end{aligned}
    $$

    which can be summarized as

    $$
    [\mathcal{J}^{(i)},\mathcal{J}^{(j)}]
    = \epsilon_{i j k} \mathcal{J}^{(k)}
    $$

- In quantum mechanics, people prefer to use

    $$
    J^{(i)}=i \mathcal{J}^{(i)}
    \quad
    (\mathcal{J}^{(i)}=-i J^{(i)})
    $$

    Show that they satisfy the commutation rules

    $$
    [J^{(i)},J^{(j)}] = i \epsilon_{i j k} J^{(k)}
    $$

    and the rotation matrices are generated by

    $$
    \begin{aligned}
        R_x(\alpha) &= \exp (-i \alpha  J^{(1)}) 
        \\
        R_y(\alpha) &= \exp (-i \alpha  J^{(2)})
        \\
        R_z(\alpha) &= \exp (-i \alpha  J^{(3)})
    \end{aligned}
    $$

## Rotation Generators under Different Basis

In the exercise above, you have proved that the rotation generators $J_i$ (now we move down the superscript and remove the brackets) satisfy

$$
[J_i,J_j] = i \epsilon_{i j k} J_k
$$

Now we want to find how do these generators transform *under rotational
change of basis*, i.e. calculate

$J_n^{\prime}=\mathcal{D}_m{}^{-1}(\alpha)J_n\mathcal{D}_m(\alpha)=\exp \left(-i \alpha  J_m\right)J_n\exp \left(+i \alpha  J_m\right)$

Recall that
$\mathcal{D}_m(\alpha)=R_m(-\alpha)=\exp \left(+i \alpha  J_m\right)$.
To proceed, we need the :

$\exp (-\alpha  B)A \exp (\alpha  B)=A+\alpha [A,B]+\frac{\alpha}{2!}[[A,B],B]+\frac{\alpha ^3}{3!}[[[A,B],B],B]+\text{...}$

For our purpose, $A=J_n,B=i J_m$. Obviously, when $m=n$, the BCH theorem
gives

$J_n^{\prime}=J_n\text{      }(m=n)$

So *the generator is invariant under rotational change of basis if they
corresponds to the same axis*.

Now we focus on the more interesting case $m\neq n$. Then

$\left[J_n,i J_m\right]=i\left[J_n,J_m\right]=i\left(i \epsilon_{n m k}J_k\right)=-\epsilon_{n m k}J_k$

For the nested commutator $\left[\left[J_n,i J_m\right],i J_m\right]$,
we note that although $m$ appears twice, we should *not* sum over it as
required by the Einstein rule. Then

$\left[\left[J_n,i J_m\right],i J_m\right]=-i \epsilon_{n m k}\left[J_k,J_m\right]=-i \epsilon_{n m k}\left(i \epsilon_{k m p}J_p\right)=\epsilon
_{n m k}\epsilon_{k m p}J_p=\epsilon_{n m k}\left(-\epsilon_{p m k}\right)J_p$

In the last equality, we used the antisymmetry of the Levi-Civita
symbol. Again, using

$\epsilon_{i j k}\epsilon_{i m n}=\delta_{j m}\delta_{k n}-\delta_{j n}\delta_{k m}$

We get (do *not* sum over $m$!)

$\left[\left[J_n,i J_m\right],i J_m\right]=-\epsilon_{n m k}\epsilon_{p m k}J_p=-\epsilon_{k n m}\epsilon_{k p m}J_p=-\left(\delta_{n p}\delta
_{m m}-\delta_{n m}\delta_{m p}\right)J_p$

Since $m\neq n$, $\delta_{n m}=0$. Therefore

$\left[\left[J_n,i J_m\right],i J_m\right]=-\delta_{n p}J_p=-J_n$

We see that we return to $J_n$ (except for a minus sign)! Then we are
able to get all the nested commutators in the BCH theorem and find

$$
\begin{array}{c}
 J_n^{\prime}=J_n-\alpha  \epsilon_{n m k}J_k-\frac{\alpha ^2}{2!}J_n+\frac{\alpha ^3}{3!}\epsilon_{n m k}J_k+\frac{\alpha ^4}{4!}J_n+\text{...}
\\
 =J_n\left(1-\frac{\alpha ^2}{2!}+\frac{\alpha ^4}{4!}-\text{...}\right)-\epsilon_{n m k}J_k\left(\alpha -\frac{\alpha ^3}{3!}+\frac{\alpha ^5}{5!}-\text{...}\right)
\\
 =J_n\cos  \alpha -\epsilon_{n m k}J_k\sin  \alpha =J_n\cos  \alpha +\epsilon_{m n k}J_k\sin  \alpha  \\
\end{array}
\text{       }(m\neq n)
$$

Let us *formally* write this result as a matrix multiplication
($1,2,3=x,y,z$)

$m=1$:

$$
\begin{aligned}
    \begin{bmatrix}
        J_1^{\prime} \\
        J_2^{\prime} \\
        J_3^{\prime} \\
    \end{bmatrix}
    &= \begin{bmatrix}
        \exp \left(-i \alpha  J_1\right)J_1\exp \left(+i \alpha  J_1\right) \\
        \exp \left(-i \alpha  J_1\right)J_2\exp \left(+i \alpha  J_1\right) \\
        \exp \left(-i \alpha  J_1\right)J_3\exp \left(+i \alpha  J_1\right) \\
    \end{bmatrix}
    \\
    &= \begin{bmatrix}
        J_1 \\
        J_2\cos  \alpha +J_3\sin  \alpha  \\
        J_3\cos  \alpha -J_2\sin  \alpha  \\
    \end{bmatrix}
    = \begin{bmatrix}
        1 & 0 & 0 \\
        0 & \cos  \alpha  & \sin  \alpha  \\
        0 & -\sin  \alpha  & \text{cos$\alpha $} \\
    \end{bmatrix}
    \begin{bmatrix}
        J_1 \\
        J_2 \\
        J_3 \\
    \end{bmatrix}
    \\
    &= R_1(-\alpha) \begin{bmatrix}
        J_1 \\
        J_2 \\
        J_3 \\
    \end{bmatrix}
\end{aligned}
$$

This is not really a matrix multiplication, but only a *shorthand* for

$$
J_n^{\prime}=\left[R_1(-\alpha)\right]_{n k}J_k
$$

$k$ appears twice, so we should sum over it. Something interesting
emerges! Similarly, we obtain

$m=2$:

$$
\begin{aligned}
    \begin{bmatrix}
        J_1^{\prime} \\
        J_2^{\prime} \\
        J_3^{\prime} \\
    \end{bmatrix}
    &= \begin{bmatrix}
        \exp \left(-i \alpha  J_2\right)J_1\exp \left(+i \alpha  J_2\right) \\
        \exp \left(-i \alpha  J_2\right)J_2\exp \left(+i \alpha  J_2\right) \\
        \exp \left(-i \alpha  J_2\right)J_3\exp \left(+i \alpha  J_2\right) \\
    \end{bmatrix}
    \\
    &=\begin{bmatrix}
        J_1\cos  \alpha -J_3\sin  \alpha  \\
        J_2 \\
        J_3\cos  \alpha +J_1\sin  \alpha  \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        \cos  \alpha  & 0 & -\sin  \alpha  \\
        0 & 1 & 0 \\
        \sin  \alpha  & 0 & \cos  \alpha  \\
    \end{bmatrix}\begin{bmatrix}
        J_1 \\
        J_2 \\
        J_3 \\
    \end{bmatrix}
    \\
    &= R_2(-\alpha)\begin{bmatrix}
        J_1 \\
        J_2 \\
        J_3 \\
    \end{bmatrix}
\end{aligned}
$$

$m=3$:

$$
\begin{aligned}
    \begin{bmatrix}
        J_1^{\prime} \\
        J_2^{\prime} \\
        J_3^{\prime} \\
    \end{bmatrix}
    &=\begin{bmatrix}
        \exp \left(-i \alpha  J_3\right)J_1\exp \left(+i \alpha  J_3\right) \\
        \exp \left(-i \alpha  J_3\right)J_2\exp \left(+i \alpha  J_3\right) \\
        \exp \left(-i \alpha  J_3\right)J_3\exp \left(+i \alpha  J_3\right) \\
    \end{bmatrix}
    \\
    &= \begin{bmatrix}
        J_1\cos  \alpha +J_2\sin  \alpha  \\
        J_2\cos  \alpha -J_1\sin  \alpha  \\
        J_3 \\
    \end{bmatrix}
    = \begin{bmatrix}
        \cos  \alpha  & \sin  \alpha  & 0 \\
        -\sin  \alpha  & \cos  \alpha  & 0 \\
        0 & 0 & 1 \\
    \end{bmatrix}
    \begin{bmatrix}
        J_1 \\
        J_2 \\
        J_3 \\
    \end{bmatrix}
    \\
    &=R_3(-\alpha)\begin{bmatrix}
        J_1 \\
        J_2 \\
        J_3 \\
    \end{bmatrix}
\end{aligned}
$$

To summarize:

$$
J_n^{\prime}=\left[R_m(-\alpha)\right]_{n k}J_k
$$

Comparing it with the behavior of the *old* basis vectors
$\boldsymbol{e}_1,\boldsymbol{e}_2,\boldsymbol{e}_3$ under the change of basis:

$$
\boldsymbol{e}_n^{\prime}
= \left[\mathcal{D}_m(\alpha)\right]_{nk}\boldsymbol{e}_k
= \left[R_m(-\alpha)\right]_{nk}\boldsymbol{e}_k
$$

We reach the important conclusion:

**The rotation generators behaves in the same way as the basis vectors of the coordinate system**.

### EXERCISE

Prove the BCH theorem using the Taylor series:

$$
f(\alpha)=f(0)+\alpha  \frac{df}{d\alpha}(0)+\frac{\alpha ^2}{2!}\frac{d^2f}{d\alpha ^2}(0)+\frac{\alpha ^3}{3!}\frac{d^3f}{d\alpha ^3}(0)+\text{...}
$$
