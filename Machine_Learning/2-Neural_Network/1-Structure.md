<style>
    .remark {
        border-radius: 15px;
        padding: 20px;
        background-color: SeaGreen;
        color: White;
    }
    .result {
        border-radius: 15px;
        padding: 20px;
        background-color: FireBrick;
        color: White;
    }
</style>

# Neural Network Structure

<div class="remark">
<center>
<i>In the following we shall NOT use Einstein summation rule. <br> All summations will be indicated explicitly.</i>
</center>
</div><br>

The **neural network** is a more sophisticated algorithm to perform multi-class classification, which also enables more complicated non-linear hypothesis. 

A typical neural network is structured as follows:

<center>
<img src="Figures/neural_Network.svg" alt="neural network" width="400px">
</center>

- The 0th layer (**input layer**) accepts the values of features $(x_1, ..., x_N)$ from the samples. Again, we add a **bias unit** $x_0 = 1$.

- The 1st and the 2nd layers (**hidden layers**) creates more complex features. The number of hidden layers needed in the network depends on the complexity of the problem, and there might be no hidden layers at all. 

    The new feature generated by the $j$th **unit** in the $l$th layer ($l = 1,2,...$) is denoted by $a_j^{l}$ (also called the **activation** of that unit). 
    
    The number of units (*excluding* the bias unit) in the $l$th layer is denoted by $N_l$. 

    We also formally define the "activation" of the input layer as

    $$
    a_j^{0} = x_j \qquad j = 0, 1, ..., N_0 \equiv N
    $$

- The final layer (**output layers**) make predictions 
    
    $$
    \begin{bmatrix}
        h_1 \\ \vdots \\ h_K
    \end{bmatrix} \equiv \begin{bmatrix}
        h_1(x) \\ \vdots \\ h_K(x)
    \end{bmatrix}
    $$ 
    
    based on the outputs from the last hidden layer. Here we use $\Theta$ to denote all parameters in the network.

## Forward Propagation

Usually, this activation of the $l$th layer is obtained by feeding a *linear combination* of the results from the last layer to some **activation function** $g$; the linear combination coefficients form a $N_l \times (N_{l-1} + 1)$ matrix $\Theta^{l}$:

<div class="result">

**For the $j$th neuron in the $l$th layer:**

$$
\begin{aligned}
    \text{Input} \quad
    z_j^l &\equiv \sum_{k=0}^{N_{l-1}}
    \Theta^{l}_{jk} a_k^{l-1}
    \\
    \text{Activation} \quad 
    a_j^{l} &= g(z_j^l)
\end{aligned}
\qquad \begin{aligned}
    l &= 1, ..., L \\
    j &= 1, ..., N_l
\end{aligned}
$$

</div><br>

Thus we obtain $a^{1}, a^{2}, ..., a^{L} = h$ successively. This process is called **forward propagation**. 

The ability of neural networks to deal with nonlinear hypothesis comes from the usage of a nonlinear activation function. For classification purposes, it is usually chosen as the sigmoid function:

$$
g(z) = \frac{1}{1 + e^{-z}}
$$

Other commonly used activation functions can be found on [Wikipedia](https://en.wikipedia.org/wiki/ActivatioN_function). 

## Simple Examples of Neural Network

### Linear Regression

### Binary Classification (Logistic Regression)

### Logical Gates (AND, OR, NOT, etc)
