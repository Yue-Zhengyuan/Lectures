# Neural Network Structure

The **neural network** is a more sophisticated algorithm to perform multi-class classification, enabling more complicated non-linear hypothesis. 

A typical neural network is structured as follows:

<center>
<img src="Figures/neural_network.svg" alt="neural network" width="400px">
</center>

- The 0th layer (**input layer**) accepts the values of features $(x_1, ..., x_n)$ from the samples. Again, we add a **bias unit** $x_0 = 1$.

- The 1st and the 2nd layers (**hidden layers**) creates more complex features. The number of hidden layers needed in the network depends on the complexity of the problem, and there might be no hidden layers at all. 

    The new feature generated by the $j$th **unit** in the $l$th layer ($l = 1,2,...$) is denoted by $a_j^{l}$ (also called the **activation** of that unit). 
    
    The number of units (*not* counting the bias unit) in the $l$th layer is denoted by $n_l$. 

    We also formally define the "activation" of the input layer as

    $$
    a_j^{(0)} = x_j \qquad j = 0, 1, ..., n_0 \equiv n
    $$

- The final layer (**output layers**) make predictions 
    
    $$
    \begin{bmatrix}
        h_1 \\ \vdots \\ h_K
    \end{bmatrix} \equiv \begin{bmatrix}
        h_1(x) \\ \vdots \\ h_K(x)
    \end{bmatrix}
    $$ 
    
    based on the outputs from the last hidden layer. Here we use $\Theta$ to denote all parameters in the network.

## Forward Propagation

Usually, this activation of the $l$th layer is obtained by feeding a *linear combination* of the results from the last layer to some **activation function** $g$; the linear combination coefficients form a $n_l \times (n_{l-1} + 1)$ matrix $\Theta^{l}$:

$$
a_j^{l} = g(\Theta^{l}_{jk} a_k^{l-1})
\qquad \begin{aligned}
    l &= 1,2,... \\
    j &= 1,..., n_l
\end{aligned}
$$

Here summation over $k = 0,1,...,n_{l-1}$ is implicitly implied. Thus we obtain $a^{1}, a^{2}, ..., h$ successively. This process is called **forward propagation**. 

The ability of neural networks to deal with nonlinear hypothesis comes from the usage of a nonlinear activation function. For classification purposes, it is usually chosen as the sigmoid function:

$$
g(z) = \frac{1}{1 + e^{-z}}
$$

Other commonly used activation functions can be found on [Wikipedia](https://en.wikipedia.org/wiki/Activation_function). 

## Simple Examples of Neural Network

### Linear Regression

### Binary Classification (Logistic Regression)

### Multi-Class Classification

In a classification problem with $K$ classes, we can create a neural network with output $(h_1, ..., h_K)$. The component $h_i$ is the result from a logistic unit (i.e. unit with logistic activation function), which means the probability that the input belongs to the $i$th class. 

When calculating each $h_i$ we are in fact implicitly doing a binary classification problem: whether the input belongs to the $i$th class or not. This method is called the **one-vs-all (one-vs-rest) algorithm**.

After obtaining the probabilities, we shall take the class that the input is *most likely to belong to*, i.e. its class is determined by the largest probability

$$
\max (h_1, ..., h_K)
$$

### Logical Gates (AND, OR, NOT, etc)
